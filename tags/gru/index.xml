<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Gru on Isaac Changhau</title>
    <link>https://isaacchanghau.github.io/tags/gru/</link>
    <description>Recent content in Gru on Isaac Changhau</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017-2018</copyright>
    <lastBuildDate>Wed, 02 Aug 2017 00:00:00 +0800</lastBuildDate>
    
	<atom:link href="https://isaacchanghau.github.io/tags/gru/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Seq2Seq Learning and Neural Conversational Model</title>
      <link>https://isaacchanghau.github.io/post/seq2seq_conversation/</link>
      <pubDate>Wed, 02 Aug 2017 00:00:00 +0800</pubDate>
      
      <guid>https://isaacchanghau.github.io/post/seq2seq_conversation/</guid>
      <description>It is a summary of two papers: Sequence to Sequence Learning with Neural Networks and A Neural Conversational Model, as well as the implementation of Neural Conversation Model via Java with deeplearning4j package.
Sequence to Sequence Model In this paper, the author presents a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. The method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector.</description>
    </item>
    
    <item>
      <title>LSTM and GRU -- Formula Summary</title>
      <link>https://isaacchanghau.github.io/post/lstm-gru-formula/</link>
      <pubDate>Sat, 22 Jul 2017 00:00:00 +0800</pubDate>
      
      <guid>https://isaacchanghau.github.io/post/lstm-gru-formula/</guid>
      <description>Introduction Long Short-Term Memory (LSTM) unit and Gated Recurrent Unit (GRU) RNNs are among the most widely used models in Deep Learning for NLP today. Both LSTM (1997) and GRU (2014) are designed to combat the vanishing gradient problem prevents standard RNNs from learning long-term dependencies through gating mechanism.
Note that, this article heavily rely on the following to articles, Understanding LSTM Networks and Recurrent Neural Network Tutorial, I summary the formula definition and explanation from them to enhance my understanding of LSTM and GRU as well as their similarity and difference.</description>
    </item>
    
    <item>
      <title>Understanding LSTM Networks</title>
      <link>https://isaacchanghau.github.io/post/understand_lstm/</link>
      <pubDate>Tue, 13 Jun 2017 00:00:00 +0800</pubDate>
      
      <guid>https://isaacchanghau.github.io/post/understand_lstm/</guid>
      <description>Declaration: This blog article is totaly not my original article. I just reproduce it from colah&amp;rsquo;s blog &amp;ndash; Understanding LSTM Networks, since it is really an excellent article of explanation of LSTM and I am afraid that I may lose the link of this article or the author may change her blog address. So I put this article into my own blog&amp;hellip; Moreover, the Chinese version of this article, translated by Not_GOD, are available here.</description>
    </item>
    
  </channel>
</rss>